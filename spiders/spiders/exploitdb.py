# -*- coding: utf-8 -*-
import logging
import pdb
import re
import json
import scrapy
import time

from spiders.common.constantFields import TYPE_URL, TYPE_ITEM, TYPE_REQUEST
from spiders.common.http_post import send_http
from spiders.spiders.base import BaseSpider

class ExploitdbSpider(BaseSpider):
    name = 'exploitdb'
    allowed_domains = ['exploit-db.com']
    start_urls = ['https://www.exploit-db.com/']
    parsePage = 'getList'
    visitCount = 1
    startIndex = 0
    pageLength = 15
    maxPageCount = 2

    custom_settings = {
        'CONCURRENT_REQUESTS': 1,
        'DOWNLOAD_DELAY': 2,
    }

    headers = {
        'authority': 'www.exploit-db.com',
        'pragma': 'no-cache',
        'cache-control': 'no-cache',
        'accept': 'application/json, text/javascript, */*; q=0.01',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36',
        'x-requested-with': 'XMLHttpRequest',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-mode': 'cors',
        'sec-fetch-dest': 'empty',
        'referer': 'https://www.exploit-db.com/',
        'accept-language': 'zh-CN,zh;q=0.9',
    }

    def getList(self, response):
        logging.info('start getList')
        metaInfo = response.meta.get('metaInfo')
        itemInfoList = []
        self.pageCount += 1  # 统计页数

        if self.visitCount == 1:
            index_url = 'https://www.exploit-db.com/?draw=' + str(self.visitCount) + '&columns%5B0%5D%5Bdata%5D=date_published&columns%5B0%5D%5Bname%5D=date_published&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=download&columns%5B1%5D%5Bname%5D=download&columns%5B1%5D%5Bsearchable%5D=false&columns%5B1%5D%5Borderable%5D=false&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=application_md5&columns%5B2%5D%5Bname%5D=application_md5&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=false&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=verified&columns%5B3%5D%5Bname%5D=verified&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=false&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=description&columns%5B4%5D%5Bname%5D=description&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=false&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=type_id&columns%5B5%5D%5Bname%5D=type_id&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=false&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=platform_id&columns%5B6%5D%5Bname%5D=platform_id&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=false&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=author_id&columns%5B7%5D%5Bname%5D=author_id&columns%5B7%5D%5Bsearchable%5D=false&columns%5B7%5D%5Borderable%5D=false&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=code&columns%5B8%5D%5Bname%5D=code.code&columns%5B8%5D%5Bsearchable%5D=true&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B9%5D%5Bdata%5D=id&columns%5B9%5D%5Bname%5D=id&columns%5B9%5D%5Bsearchable%5D=false&columns%5B9%5D%5Borderable%5D=true&columns%5B9%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B9%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=9&order%5B0%5D%5Bdir%5D=desc&start='+ str(self.startIndex) + '&length='+ str(self.pageLength) +'&search%5Bvalue%5D=&search%5Bregex%5D=false&author=&port=&type=&tag=&platform=&_=' + str(time.time()).replace('.', '')[0:13]
            request = scrapy.Request(url=index_url, headers=self.headers)
            urlInfo = {
                'itemType': TYPE_REQUEST,
                'parsePage': 'getList',
                'metaInfo': metaInfo,
                'item': request,
                'dont_filter': True,
            }
            itemInfoList.append(urlInfo)
            self.visitCount += 1

        if 'draw=' not in response.url:
            return itemInfoList
        itemData = json.loads(response.text).get('data')
        for cve_index, cveItemSel in enumerate(itemData):
            cveItemUrl = 'https://www.exploit-db.com/exploits/%s' % cveItemSel.get('id')
            pubTime = cveItemSel.get('date_published').strip()

            if self.pageCount == 1 and cve_index == 0:  # 记录当天最新数据
                self.today_latest_item_data = {
                    'url': cveItemUrl,
                    'pubTime': pubTime
                }
                logging.info('lastest data is %s' % json.dumps(self.today_latest_item_data))

            if cveItemUrl == self.latestDataInfo.get('url') and pubTime == self.latestDataInfo.get(
                    'pubTime'):  # 根据时间和url进行判断是否为新数据
                logging.info('find history data, stop spider')
                break

            urlInfo = {
                'itemType': TYPE_URL,
                'parsePage': 'getCveItemInfo',
                'metaInfo': metaInfo,
                'item': cveItemUrl,
            }
            itemInfoList.append(urlInfo)

            # 测试打开
            # break
        else:
            # next page
            self.startIndex += self.pageLength
            nextPageUrl = 'https://www.exploit-db.com/?draw=' + str(self.visitCount) + '&columns%5B0%5D%5Bdata%5D=date_published&columns%5B0%5D%5Bname%5D=date_published&columns%5B0%5D%5Bsearchable%5D=true&columns%5B0%5D%5Borderable%5D=true&columns%5B0%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B0%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B1%5D%5Bdata%5D=download&columns%5B1%5D%5Bname%5D=download&columns%5B1%5D%5Bsearchable%5D=false&columns%5B1%5D%5Borderable%5D=false&columns%5B1%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B1%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B2%5D%5Bdata%5D=application_md5&columns%5B2%5D%5Bname%5D=application_md5&columns%5B2%5D%5Bsearchable%5D=true&columns%5B2%5D%5Borderable%5D=false&columns%5B2%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B2%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B3%5D%5Bdata%5D=verified&columns%5B3%5D%5Bname%5D=verified&columns%5B3%5D%5Bsearchable%5D=true&columns%5B3%5D%5Borderable%5D=false&columns%5B3%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B3%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B4%5D%5Bdata%5D=description&columns%5B4%5D%5Bname%5D=description&columns%5B4%5D%5Bsearchable%5D=true&columns%5B4%5D%5Borderable%5D=false&columns%5B4%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B4%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B5%5D%5Bdata%5D=type_id&columns%5B5%5D%5Bname%5D=type_id&columns%5B5%5D%5Bsearchable%5D=true&columns%5B5%5D%5Borderable%5D=false&columns%5B5%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B5%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B6%5D%5Bdata%5D=platform_id&columns%5B6%5D%5Bname%5D=platform_id&columns%5B6%5D%5Bsearchable%5D=true&columns%5B6%5D%5Borderable%5D=false&columns%5B6%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B6%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B7%5D%5Bdata%5D=author_id&columns%5B7%5D%5Bname%5D=author_id&columns%5B7%5D%5Bsearchable%5D=false&columns%5B7%5D%5Borderable%5D=false&columns%5B7%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B7%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B8%5D%5Bdata%5D=code&columns%5B8%5D%5Bname%5D=code.code&columns%5B8%5D%5Bsearchable%5D=true&columns%5B8%5D%5Borderable%5D=true&columns%5B8%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B8%5D%5Bsearch%5D%5Bregex%5D=false&columns%5B9%5D%5Bdata%5D=id&columns%5B9%5D%5Bname%5D=id&columns%5B9%5D%5Bsearchable%5D=false&columns%5B9%5D%5Borderable%5D=true&columns%5B9%5D%5Bsearch%5D%5Bvalue%5D=&columns%5B9%5D%5Bsearch%5D%5Bregex%5D=false&order%5B0%5D%5Bcolumn%5D=9&order%5B0%5D%5Bdir%5D=desc&start='+ str(self.startIndex) + '&length='+ str(self.pageLength) +'&search%5Bvalue%5D=&search%5Bregex%5D=false&author=&port=&type=&tag=&platform=&_=' + str(time.time()).replace('.', '')[0:13]
            self.visitCount += 1
            request = scrapy.Request(url=nextPageUrl, headers=self.headers)
            urlInfo = {
                'itemType': TYPE_REQUEST,
                'parsePage': 'getList',
                'metaInfo': metaInfo,
                'item': request,
                'dont_filter': True,
            }

            if self.pageCount <= self.maxPageCount:  # 防止出错停止不了
                itemInfoList.append(urlInfo)
            else:
                logging.info('stop spider mandatory, spider page count is %d' % self.maxPageCount)

        return itemInfoList

    def getCveItemInfo(self, response):
        logging.info('start getCveItemInfo')
        metaInfo = response.meta.get('metaInfo')
        itemInfoList = []

        edbId = ''
        cveCode = ''
        author = ''
        cveUseType = ''
        platform = ''
        pubTime = ''
        edbVerified = ''
        pocDownloadUrl = ''
        pocContent = ''.join(response.xpath('//code//text()').extract())
        cveSource = 'EXPLOIT-DB'
        cveTitle = response.xpath('//h1[contains(@class,"card-title")]/text()').extract()[0]

        infoSelList = response.xpath('//div[@class="col-6 text-center"]')
        for infoSel in infoSelList:
            infoStr = infoSel.xpath('./h4/text()').extract()[0].strip()
            if 'EDB-ID:' in infoStr:
                edbId = infoSel.xpath('./h6/text()').extract()[0].strip()
            if 'CVE:' in infoStr:
                if infoSel.xpath('./h6/a'):
                    cveCode = 'cve-' + infoSel.xpath('./h6/a/text()').extract()[0].strip()
            if 'Author:' in infoStr:
                author = infoSel.xpath('./h6/a/text()').extract()[0].strip()
            if 'Type:' in infoStr:
                cveUseType = infoSel.xpath('./h6/a/text()').extract()[0].strip()
            if 'Platform:' in infoStr:
                platform = infoSel.xpath('./h6/a/text()').extract()[0].strip()
            if 'Date:' in infoStr:
                pubTime = infoSel.xpath('./h6/text()').extract()[0].strip()

        infoExtSelList = response.xpath('//div[@class="stats h5 text-center"]')
        for infoExtSel in infoExtSelList:
            infoExtStr = infoExtSel.xpath('./strong/text()').extract()[0].strip()
            # if 'EDB Verified:' in infoExtStr:
            #     edbId = infoExtSel.xpath('./h6/text()').extract()[0].strip()
            if 'Exploit:' in infoExtStr:
                pocDownloadUrl = response.urljoin(infoExtSel.xpath('./a/@href').extract()[0].strip())
            # if 'Vulnerable App:' in infoExtStr: # todo 待调研
            #     vulnerableApp = infoExtSel.xpath('./a/@href').extract()[0].strip()

        item = {}
        item["cveItemUrl"] = response.url
        item["pocScript"]= pocContent
        item["cveSource"]= cveSource
        item["cveItemTitle"]= cveTitle
        item["sourceId"]= edbId
        item["cveCode"]= cveCode
        item["author"]= author
        item["cveUseType"]= cveUseType
        item["affectedSystem"]= platform
        item["pubTime"]= pubTime
        item["isverify"]= edbVerified
        item["pocDownloadUrl"]= pocDownloadUrl
        urlInfo = {
            'itemType': TYPE_ITEM,
            'item': item,
        }
        itemInfoList.append(urlInfo)
        return itemInfoList

# 运行命令：scrapy crawl exploitdb -a taskType='spider' -a taskId=1 -o data.csv
# 调试或者部分抓取：scrapy crawl exploitdb -a taskType='update' -a taskId=1 -a sourceUrls='["https://www.ibm.com/blogs/psirt/security-bulletin-cross-site-scripting-vulnerability-affect-ibm-business-automation-workflow-and-ibm-business-process-manager-bpm-cve-2020-4698-2/"]'
